## Citas


@book{mitchell_learn,
 author = {Mitchell, Thomas M.},
 title = {Machine Learning},
 year = {1997},
 isbn = {0070428077, 9780070428072},
 edition = {1},
 publisher = {McGraw-Hill, Inc.},
 address = {New York, NY, USA},
}

@book{test_val,
 author = {Russell, Stuart and Norvig, Peter},
 title = {Artificial Intelligence: A Modern Approach},
 year = {2009},
 isbn = {0136042597, 9780136042594},
 edition = {3rd},
 publisher = {Prentice Hall Press},
 address = {Upper Saddle River, NJ, USA},
}

@article{semi_supervised,
  title={Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]},
  author={Chapelle, Olivier and Scholkopf, Bernhard and Zien, Alexander},
  journal={IEEE Transactions on Neural Networks},
  volume={20},
  number={3},
  pages={542--542},
  year={2009},
  publisher={IEEE}
}


@inproceedings{supervised_learning,
 author = {Caruana, Rich and Niculescu-Mizil, Alexandru},
 title = {An Empirical Comparison of Supervised Learning Algorithms},
 booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
 series = {ICML '06},
 year = {2006},
 isbn = {1-59593-383-2},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {161--168},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1143844.1143865},
 doi = {10.1145/1143844.1143865},
 acmid = {1143865},
 publisher = {ACM},
 address = {New York, NY, USA},
}


@incollection{unsupervised_learning,
  title={Unsupervised learning},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  booktitle={The elements of statistical learning},
  pages={485--585},
  year={2009},
  publisher={Springer}
}


Buen material de SVM: http://mcminis1.github.io/blog/2014/05/10/intuition-for-SVR/

@article{random_forest,
  title={Random forests},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={45},
  number={1},
  pages={5--32},
  year={2001},
  publisher={Springer}
}


@article{monte_carlo,
  title={The monte carlo method},
  author={Metropolis, Nicholas and Ulam, Stanislaw},
  journal={Journal of the American statistical association},
  volume={44},
  number={247},
  pages={335--341},
  year={1949},
  publisher={Taylor \& Francis}
}


@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{sklearn_review,
  title={API design for machine learning software: experiences from the scikit-learn project},
  author={Buitinck, Lars and Louppe, Gilles and Blondel, Mathieu and Pedregosa, Fabian and Mueller, Andreas and Grisel, Olivier and Niculae, Vlad and Prettenhofer, Peter and Gramfort, Alexandre and Grobler, Jaques and others},
  journal={arXiv preprint arXiv:1309.0238},
  year={2013}
}


@Misc{scipy,
  author = {Eric Jones and Travis Oliphant and Pearu Peterson and others},
  title = {{SciPy}: Open source scientific tools for {Python}},
  year = {2001--},
  url = "http://www.scipy.org/",
  note = {[Online; accessed <today>]}
}


@incollection{backpropagation,
  title={Theory of the backpropagation neural network},
  author={Hecht-Nielsen, Robert},
  booktitle={Neural networks for perception},
  pages={65--93},
  year={1992},
  publisher={Elsevier}
}

@ARTICLE{mlp_intro1,
author={D. W. Ruck and S. K. Rogers and M. Kabrisky and M. E. Oxley and B. W. Suter},
journal={IEEE Transactions on Neural Networks},
title={The multilayer perceptron as an approximation to a Bayes optimal discriminant function},
year={1990},
volume={1},
number={4},
pages={296-298},
keywords={neural nets;probability;multiple class problems;neural networks;multilayer perceptron;Bayes optimal discriminant function;classifier;backpropagation;two-class problem;probability;unit activation function;Multilayer perceptrons;Probability density function;Backpropagation;Pattern recognition;Bayesian methods;Books;Image analysis;Multi-layer neural network;Neural networks},
doi={10.1109/72.80266},
ISSN={1045-9227},
month={Dec},}


@article{mlp_intro2,
author = {Nazzal, Jamal and M. El-Emary, Ibrahim and A. Najim, Salam},
year = {2008},
month = {01},
pages = {},
title = {Multilayer Perceptron Neural Network (MLPs) For Analyzing the Properties of Jordan Oil Shale},
volume = {5},
booktitle = {World Applied Sciences Journal}
}



@article{decision_tree_regression,
title = "Decision tree regression for soft classification of remote sensing data",
journal = "Remote Sensing of Environment",
volume = "97",
number = "3",
pages = "322 - 336",
year = "2005",
issn = "0034-4257",
doi = "https://doi.org/10.1016/j.rse.2005.05.008",
url = "http://www.sciencedirect.com/science/article/pii/S0034425705001604",
author = "Min Xu and Pakorn Watanachaturaporn and Pramod K. Varshney and Manoj K. Arora",
keywords = "Non-parametric classification, Decision tree regression, Soft classification, Classification accuracy"
}


@inproceedings{sklearn_api,
  author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
               Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
               Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
               and Jaques Grobler and Robert Layton and Jake VanderPlas and
               Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
  title     = {{API} design for machine learning software: experiences from the scikit-learn
               project},
  booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
  year      = {2013},
  pages = {108--122},
}


@MISC{svr_tutorial,
    author = {Alex J. Smola and Bernhard Schölkopf},
    title = {A tutorial on support vector regression },
    year = {2004}
}

@article{review_svr,
  title={Support vector regression},
  author={Basak, Debasish and Pal, Srimanta and Patranabis, Dipak Chandra},
  journal={Neural Information Processing-Letters and Reviews},
  volume={11},
  number={10},
  pages={203--224},
  year={2007}
}


@inproceedings{support_vector_regression,
  title={Support vector regression machines},
  author={Drucker, Harris and Burges, Christopher JC and Kaufman, Linda and Smola, Alex J and Vapnik, Vladimir},
  booktitle={Advances in neural information processing systems},
  pages={155--161},
  year={1997}
}


@inproceedings{first_svm,
 author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
 title = {A Training Algorithm for Optimal Margin Classifiers},
 booktitle = {Proceedings of the Fifth Annual Workshop on Computational Learning Theory},
 series = {COLT '92},
 year = {1992},
 isbn = {0-89791-497-X},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {144--152},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/130385.130401},
 doi = {10.1145/130385.130401},
 acmid = {130401},
 publisher = {ACM},
 address = {New York, NY, USA},
}


tag: least_square
Goldberger, Arthur S. (1964). "Classical Linear Regression". Econometric Theory. New York: John Wiley & Sons. pp. 156–212 [p. 158]. ISBN 0-471-31101-4.


tag: knnr
Altman, N. S. (1992). "An introduction to kernel and nearest-neighbor nonparametric regression". The American Statistician. 46 (3): 175–185. doi:10.1080/00031305.1992.10475879.

## Ideas

- Hablar del machine Learning en general, un poco de historia y
de tecnisismos  hasta la actualidad.
- Hablar un poco de machine learning en aplicaciones sociales.
- Hablar de machine learning en Geociencias
------------- parte técnica ----------
- Scikit learn ✓✓
- Desarrollar más en profundidad:
  - Regresión Lineal:
        - común ✓
        - ridge ✓
  - SVR ✓✓
  - Multilayer perceptron ✓
  - KNN ✓✓
  - DTR ✓✓
  - Random Forest ✓✓
- Irace



### SciKit learn

This project was started in 2007 as a Google Summer of Code project by David Cournapeau. Later that year, Matthieu
Brucher started work on this project as part of his thesis.
In 2010 Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort and Vincent Michel of INRIA took leadership of the
project and made the first public release, February the 1st 2010. Since then, several releases have appeared following
a ~3 month cycle, and a thriving international community has been leading the development

------


scikit-learn is a free-to-use machine learning module for Python built on SciPy. It is a straightforward and effective tool for data mining and data analysis. Because it is released with a BSD license, it can be used for both personal and commercial reasons.

With scikit-learn, users are able to conduct a variety of tasks under different categories like model selection, clustering, preprocessing, and more. The module provides the means to complete implementations.

Moreover, scikit-learn has an extensive use. It is being utilized by big companies in different industries like music streaming, hotel bookings, and more. This means that users can integrate algorithms in the platform to their own applications.


Free Platform

Because scikit-learn is released with a BSD license, it can be used for free by everyone. This license has minimal restrictions; therefore, users can utilize it to design their applications and platforms with little worry over limitations.

Industrial Use

scikit-learn is a helpful platform that can predict consumer behavior, identify abusive actions in the cloud, create neuroimages, and more. It is being used extensively by commercial and research organizations around the world, a testament to its ease of use and overall advantage.

Collaborative Library

scikit-learn began as a one-man mission but now it is being built by numerous authors from INRIA spearheaded by Fabian Pedregosa and individual contributors who are not attached to teams or organizations. This makes the module a well-updated one, releasing updates several times a year. Users can also look forward to assistance from an international community, in case they have queries or if they hit snags in development using the module.

Ease of Use

Commercial entities and research organizations alike have employed scikit-learn in their processes. They all agree that the module is easy-to-use, thereby allowing them to perform a multitude of processes with nary a problem.

API Documentation

scikit-learn ensures that users old and new alike get the assistance they need in integrating the machine learning module into their own platforms. That is why a documentation detailing the use of its API exists that users can access anytime on the website. This makes certain developers can implement machine learning algorithms offered by the tool seamlessly.

### Multilayer Perceptron
http://scikit-learn.org/stable/modules/neural_networks_supervised.html#mathematical-formulation

A multilayer perceptron (MLP) is a class of feedforward artificial neural network. An MLP consists of at least three layers of nodes. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training.[1][2] Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.[3]

Multi-layer Perceptron (MLP) is a supervised learning algorithm that learns a function f(\cdot): R^m \rightarrow R^o by training on a dataset, where m is the number of dimensions for input and o is the number of dimensions for output. Given a set of features X = {x_1, x_2, ..., x_m} and a target y, it can learn a non-linear function approximator for either classification or regression. It is different from logistic regression, in that between the input and the output layer, there can be one or more non-linear layers, called hidden layers. Figure 1 shows a one hidden layer MLP with scalar output.

The leftmost layer, known as the input layer, consists of a set of neurons \{x_i | x_1, x_2, ..., x_m\} representing the input features. Each neuron in the hidden layer transforms the values from the previous layer with a weighted linear summation w_1x_1 + w_2x_2 + ... + w_mx_m, followed by a non-linear activation function g(\cdot):R \rightarrow R - like the hyperbolic tan function. The output layer receives the values from the last hidden layer and transforms them into output values.

### Decision Tree
Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.


Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node (e.g., Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy), each representing values for the attribute tested. Leaf node (e.g., Hours Played) represents a decision on the numerical target. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data.

http://scikit-learn.org/stable/modules/tree.html#regression-criteria

ahí están las citas también



### Random Forest
http://www.jmlr.org/papers/volume13/biau12a/biau12a.pdf  Página 2

### KNN (Regression)
The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice.


Predictions are made for a new instance (x) by searching through the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances. For regression this might be the mean output variable, in classification this might be the mode (or most common) class value.

When KNN is used for regression problems the prediction is based on the mean or the median of the K-most similar instances

poner ecuación

y^estimada(x) = 1/k * sumatoria(y_j tal que j \in knn(x))
y^estimada es la y con ~ arriba

### SVM
A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier.

svm_hiperplane.png

#### SVR
Support vector regression (SVR) is a fast and accurate way of interpolating data sets. It is useful when you have an expensive function you want to approximate over a known domain. It learns quickly and is systematically improvable. Variants of SVR are used throughout science including Krigging and Gaussian processes (GP).

Support vector regression is a generalization of the support vector machine to the regression problem. Technically, it can be labelled as a supervised learning algorithm. It requires a training set, \(\mathcal{T} = \{ \vec{X}, \vec{Y} \}\) which covers the domain of interest and is accompanied by solutions on that domain. The work of the SVM is to approximate the function we used to generate the training set, \[ F(\vec{X}) = \vec{Y}\]. In my mind, it's just an interpolation scheme.

In a classification problem, the vectors \(\vec{X}\) are used to define a hyperplane that seperates the two different classes in your solution. In SVR, these vectors are used to perform linear regression. The vectors closest to your test point, or decision boundary are the ones we refer to as support vectors. For regression, it turns out that all of the vectors are support vectors. We can evaluate our function anywhere, so any vectors could be closest to our test evaluation location.

http://scikit-learn.org/stable/modules/svm.html#svr



### Lineales

#### Ordinary least squares Linear Regression.
https://en.wikipedia.org/wiki/Ordinary_least_squares#Simple_linear_regression_model

#### Ridge
Ridge Regression
is a technique for analyzing multiple regression data that suffer from multicollinearity. When
multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from
the true value. By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors.
It is hoped that the net effect will be to give estimates that are more reliable. Another biased regression technique,
principal components regression, is also available in
NCSS
. Ri
dge regression is the more popular of the two
methods

https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Ridge_Regression.pdf página 3
http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression
